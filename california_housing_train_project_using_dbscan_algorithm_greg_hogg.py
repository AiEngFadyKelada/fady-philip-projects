# -*- coding: utf-8 -*-
"""California_Housing_Train Project using DBSCAN Algorithm Greg Hogg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IQDPEgZu1AeETtM1QsO4aAc-voQLcknA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN

df=pd.read_csv('sample_data/california_housing_train.csv')
df

df.describe()

df.isnull().sum()

df.info()

coordinate_df=df[['latitude','longitude']]
long,lat=coordinate_df.longitude,coordinate_df.latitude
plt.scatter(long,lat)
plt.show()

X=coordinate_df.values
X.shape
DB_model=DBSCAN(eps=.2,min_samples=15)
DB_model.fit(X)

DB_model.labels_

DB_model.labels_.shape

df['Cluster Group']=DB_model.labels_
df

df['Cluster Group']

"""# (-1) value means noise or outliers, not a cluster !!"""

import plotly.express as px
fig=px.scatter(x=long,y=lat,color=DB_model.labels_)
fig.show()

from sklearn.metrics import silhouette_score as sh
sh(X,df["Cluster Group"])

epsilons=np.linspace(0.1,1,num=15)
print(epsilons)
min_samples=np.arange(2,20,step=3)
print(min_samples)

import itertools
combinations=list(itertools.product(epsilons,min_samples))
print(combinations)
n=len(combinations)
print(n)

def get_scores_and_labels(combinations, X):
  scores = []
  labels2 = []
  i=0
  for eps, min_sample in combinations:
    DB_model = DBSCAN(eps=eps, min_samples=min_sample).fit(X)
    labels = DB_model.labels_
    labels_set = set(labels)
    num_clusters = len(labels_set)
    if -1 in labels:
      num_clusters -= 1

    if (num_clusters < 2) or (num_clusters > 50):
      scores.append(-10)
      labels2.append('Garbage')
      c = (eps,min_sample)
      i+=1
      print(f"Combination {c} on iteration {i} of {n} has {num_clusters} clusters , and we are Moving on.... !!")
      continue

    scores.append(sh(X, labels))
    labels2.append(labels)

  best_index = np.argmax(scores)
  best_parameters = combinations[best_index]
  best_labels = labels2[best_index]
  best_score = scores[best_index]

  return {'best_epsilon': best_parameters[0],
          'best_min_samples': best_parameters[1],
          'best_labels': best_labels,
          'best_score': best_score}


best_dict = get_scores_and_labels(combinations, X)

print(best_dict)

df['Cluster Group']=best_dict["best_labels"]
df['Cluster Group'].value_counts()

fig=px.scatter(x=long,y=lat,color=df['Cluster Group'])
fig.show()

df

X2=df[['total_rooms','housing_median_age']]

best_dict2 = get_scores_and_labels(combinations, X2)

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X2_scaled=scaler.fit_transform(X2)
plt.scatter(X2_scaled[:,0],X2_scaled[:,1])
plt.show()

best_dict2_scaled=get_scores_and_labels(combinations, X2_scaled)











