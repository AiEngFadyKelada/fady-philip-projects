# -*- coding: utf-8 -*-
"""Rooms classifaction project using MobileNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rMi--0-SuQ7lBu0NJafxNgI4mf-BIgxh

#**AlexNet model on Rooms dataset**

#1.Install libraries and load data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import models,layers,Sequential
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout,BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import cv2
import os

!pip install kaggle
import json
from zipfile import ZipFile
kaggle_credentails = json.load(open("kaggle.json"))
os.environ['KAGGLE_USERNAME'] = kaggle_credentails["username"]
os.environ['KAGGLE_KEY'] = kaggle_credentails["key"]
!kaggle datasets download -d robinreni/house-rooms-image-dataset
!ls
# Unzip the downloaded dataset
with ZipFile("house-rooms-image-dataset.zip", 'r') as zip_ref:
    zip_ref.extractall()

print(os.listdir("House_Room_Dataset"))

"""#**2.Data Preprocessing and EDA**"""

#Create a Dataframe of images names


#define paths
print(os.listdir("House_Room_Dataset"))
dataset_path=os.listdir("House_Room_Dataset")
print(dataset_path)
room_types=os.listdir("House_Room_Dataset")
remove=['Kitchen', 'Bathroom']
room_types=[i for i in room_types if i not in remove]
print(room_types)

rooms=[]
for item in room_types:
  #get all files' names in 6 different lists
  all_rooms=os.listdir("House_Room_Dataset"+"/"+item)
  print(all_rooms)
  #add all files' names that are in 6 lists to only one list called rooms
  for room in all_rooms:
    rooms.append((item,str("House_Room_Dataset")+"/"+item+"/"+room))
print(rooms)
print(len(rooms))

#build dataframe
df=pd.DataFrame(data=rooms,columns=['room_type','image_path'])
print(df)
print('\n\n\n')
#Exploratory Data Analysis for labels of dataset
print("total number of rooms in the dataset:",len(df))
print('\n\n')
room_count=df['room_type'].value_counts()
print("Rooms in Each Category:")
print(room_count)

"""#resize images and create list of data for the images and list for  their labels too"""

# resizing images
path="House_Room_Dataset"
base_dir="House_Room_Dataset/"
im_size=224
batch_size=16
images=[]
labels=[]
for i in room_types:
  data_path=path+"/"+i
  filenames=[i for i in os.listdir(data_path)]
  for f in filenames:
    img=cv2.imread(data_path+'/'+f)
    img=cv2.resize(img,(im_size,im_size))
    images.append(img)
    labels.append(i)

images

labels

"""#3.Data Augmentation   (train test split,rescale data)"""



#transform the image array into a numpy type
images=np.array(images)
images.shape

#rescaling images (by rescaling pixel values in range [0:1])
images=images.astype('float32')/255
images

#**Handling categorical data using Label Encoder,and OneHotEncoder**"""

y=df['room_type'].values
print(y)
LE=LabelEncoder()
y=LE.fit_transform(y)
print(y)
y=y.reshape(-1,1)
OHE=OneHotEncoder(categories='auto')
y=OHE.fit_transform(y).toarray()
print(y)
print(y.shape)


from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Augmentation for the training data
train_data_gen = ImageDataGenerator(
    rescale=1/255,                # Normalize pixel values to [0, 1]
    rotation_range=20,            # Rotate images up to 20 degrees
    width_shift_range=0.2,        # Shift width by 20%
    height_shift_range=0.2,       # Shift height by 20%
    shear_range=0.15,             # Apply shearing transformations
    zoom_range=0.2,               # Randomly zoom in/out
    brightness_range=[0.8, 1.2],  # Adjust brightness
    horizontal_flip=True,         # Flip images horizontally
    fill_mode='nearest',          # Fill missing pixels after transformations
    validation_split=0.2          # Reserve 20% of the data for validation
)

# Minimal augmentation for validation data (only rescaling)
val_data_gen = ImageDataGenerator(
    rescale=1/255,                # Normalize pixel values to [0, 1]
    validation_split=0.5          # Same validation split as training generator
)

# Training generator with augmentations
train_generator = train_data_gen.flow_from_dataframe(
    dataframe=df,                  # DataFrame containing image paths and labels
    x_col='image_path',            # Column with image file paths
    y_col='room_type',             # Column with labels
    target_size=(224, 224),        # Resize images to 224x224
    class_mode='categorical',      # Multi-class classification
    batch_size=16,                 # Number of images per batch
    subset='training',             # Use as training data
    shuffle=True                   # Shuffle data for better training
)

# Validation generator with minimal augmentation
validation_generator = val_data_gen.flow_from_dataframe(
    dataframe=df,
    x_col='image_path',
    y_col='room_type',
    target_size=(224, 224),
    class_mode='categorical',
    batch_size=16,
    subset='validation',          # Use as validation data
    shuffle=False                 # No shuffle for consistent validation
)

"""#**4.Build Deep Learning Model Architecture**"""

from tensorflow.keras.applications import MobileNet

base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base model

model = Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(1024, activation='relu', kernel_regularizer=l2(0.01)),
    layers.Dropout(0.5),
    layers.Dense(512, activation='relu', kernel_regularizer=l2(0.01)),
    layers.Dropout(0.5),
    layers.Dense(3, activation='softmax')  # Adjust the number of classes
])

"""#**5.Compile the Model**"""

from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.0003)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

"""#**6.Train the Model**"""

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau
from sklearn.utils.class_weight import compute_class_weight
import numpy as np


# Set EarlyStopping callback with patience=8


# Enhanced Callbacks
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
early_stopping = EarlyStopping(monitor='val_loss', patience=14, restore_best_weights=True)
# Add callbacks to training

# Train the model
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=50,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    callbacks=[lr_scheduler, early_stopping]
)

"""#**#7. Evaluate the model**"""
print("Evaluating model...")
train_loss, train_accuracy = model.evaluate(train_generator, steps=steps_per_epoch)
print(f"Validation Accuracy: {train_accuracy * 100:.2f}%")

print("Evaluating model...")
val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)
print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")


"""#**#8. make predictions using the model**"""

predictions = model.predict(validation_generator)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = validation_generator.classes

# Compare true and predicted labels
misclassified = np.where(predicted_classes != true_classes)

