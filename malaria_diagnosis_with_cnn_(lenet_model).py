# -*- coding: utf-8 -*-
"""Malaria_Diagnosis_with_CNN_(LeNet_Model).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vKgCs_-yPUuEfKnkEJP0DyvuoWpY0ge4
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from tensorflow import keras
import tensorflow_datasets as tfds

"""#Data Preparation
Data Loading
"""

dataset,dataset_info=tfds.load('malaria',with_info=True,as_supervised=True,shuffle_files=True,split=['train'])

dataset

dataset_info

"""dataset=tf.data.Dataset.range(10)
dataset=dataset.skip(6)
list(dataset.as_numpy_iterator())

"""dataset=tf.data.Dataset.range(6)
dataset=dataset.take(6)
list(dataset.as_numpy_iterator())

"""Train_Ratio=0.6
Val_Ratio=0.2
Test_Ratio=0.2
dataset=tf.data.Dataset.range(10)
print(list(dataset.as_numpy_iterator()))
data_len=len(dataset)
train_dataset=dataset.take(int(Train_Ratio*data_len))
print(list(train_dataset.as_numpy_iterator()))
val_dataset=dataset.skip(int(Train_Ratio*data_len))
val_dataset=val_dataset.take(int(Val_Ratio*data_len))
print(list(val_dataset.as_numpy_iterator()))
test_dataset=dataset.skip(int((Val_Ratio+Train_Ratio)*data_len))
print(list(test_dataset.as_numpy_iterator()))

def splits(dataset,Train_Ratio,Val_Ratio,Test_Ratio):
  data_len=len(dataset)
  train_dataset=dataset.take(int(Train_Ratio*data_len))
  val_dataset=dataset.skip(int(Train_Ratio*data_len))
  val_dataset=val_dataset.take(int(Val_Ratio*data_len))
  test_dataset=dataset.skip(int((Val_Ratio+Train_Ratio)*data_len))
  return train_dataset,val_dataset,test_dataset

# In cell 17
Train_Ratio=0.8
Val_Ratio=0.1
Test_Ratio=0.1

#dataset = tf.data.Dataset.range(10)

# Use the entire dataset, not a single element
train_dataset, val_dataset, test_dataset = splits(dataset[0], Train_Ratio, Val_Ratio, Test_Ratio)

print(list(train_dataset.take(1).as_numpy_iterator()))
print(list(val_dataset.take(1).as_numpy_iterator()))
print(list(test_dataset.take(1).as_numpy_iterator()))

"""#Data Visiualization"""

for i,(image,label)in enumerate(train_dataset.take(16)):
  pics=plt.subplot(4,4,i+1)
  plt.imshow(image)
  plt.title(dataset_info.features['label'].int2str(label))
  plt.axis('off')

print(dataset_info.features['label'].int2str(0))
print(dataset_info.features['label'].int2str(1))

"""#Data Preprocessing
1.resizing all images with different width and height into images with same width and height (ex.224*224)

2.normalization process ( all the data fall in the same given range(0,1) ) by dividing x over 225
"""

im_size=224
def resize_rescale(image,label):
  resized_im=tf.image.resize(image,(im_size,im_size))/255
  return resized_im,label

train_dataset=train_dataset.map(resize_rescale)
val_dataset=val_dataset.map(resize_rescale)
test_dataset=test_dataset.map(resize_rescale)

for image,label in train_dataset.take(1):
  print(image,label)

batch_size=32
train_dataset=train_dataset.shuffle(buffer_size=8,reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)
val_dataset=val_dataset.shuffle(buffer_size=8,reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)
test_dataset=test_dataset.shuffle(buffer_size=8,reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)

"""#Model Creation"""

"""model=tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(im_size,im_size,3)),
    tf.keras.layers.Conv2D(filters=6,kernel_size=5,strides=1,padding='valid',activation='sigmoid'),
    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),
    tf.keras.layers.Conv2D(filters=16,kernel_size=5,strides=1,padding='valid',activation='sigmoid'),
    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(100,activation='sigmoid'),
    tf.keras.layers.Dense(10,activation='sigmoid'),
    tf.keras.layers.Dense(1,activation='sigmoid')])
model.summary()

"""optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

"""cost_history=model.fit(train_dataset,validation_data=val_dataset,epochs=100,verbose=1)

LeNet_model=tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(im_size,im_size,3)),
    tf.keras.layers.Conv2D(filters=6,kernel_size=3,strides=1,padding='valid',activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),
    tf.keras.layers.Conv2D(filters=16,kernel_size=3,strides=1,padding='valid',activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(100,activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10,activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(1,activation='sigmoid')])
LeNet_model.summary()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
LeNet_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

cost_history=LeNet_model.fit(train_dataset,validation_data=val_dataset, epochs=20,verbose=1)

LeNet_model.evaluate(test_dataset)



