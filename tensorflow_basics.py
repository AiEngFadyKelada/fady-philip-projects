# -*- coding: utf-8 -*-
"""Tensorflow Basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iukhkeRMknFRjfTm5LY8wZ-eeorG2p2T
"""

import tensorflow as tf
tf.compat.v1.disable_eager_execution()
session=tf.compat.v1.Session()

with session.graph.as_default():
  session=tf.compat.v1.Session()
  a=tf.constant(2)
  b=tf.constant(3)
  c=a*b
  result=session.run(c)
  print(result)

with session.graph.as_default():
 session=tf.compat.v1.Session()
 x1=tf.constant([1,2,3,4])
 x2=tf.constant([5,6,7,8])
 result1=x1*x2
 result2=session.run(result1)
 print(result2)

with session.graph.as_default():
 a=tf.compat.v1.placeholder(tf.float32)
 b=tf.compat.v1.placeholder(tf.float32)
 y=a*b
 res=session.run(y,{a:[1,2,3],b:[4,5,6]})
print(res)

with session.graph.as_default():
  a=tf.compat.v1.placeholder(tf.float64)
  b=tf.compat.v1.placeholder(tf.float64)
  y=a*b
  res=session.run(y,{a:[1,2,3],b:[5,6,7]})
  print(res)

res2=session.run(y,{a:[3,4,5],b:[6,7,8]})
print(res2)

a=tf.compat.v1.Variable([2])
b=tf.compat.v1.Variable([3])
y=a*b
init=tf.compat.v1.global_variables_initializer()
session.run(init)
result=session.run(y)
print(result)

"""#Single Layer Perceptron Implementation"""

w=tf.Variable([.2])
b=tf.Variable([-0.2])
x=tf.compat.v1.placeholder(tf.float32)
linear_model=w*x+b
init=tf.compat.v1.global_variables_initializer()
session.run(init)
print("predicted output is :",session.run(linear_model,{x:[1,2,3,4]}))

y=tf.compat.v1.placeholder(tf.float32) #Actual output
squared_deltas=tf.square(linear_model-y)
loss=tf.reduce_sum(squared_deltas)
print("loss value is :",session.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))

"""#**Task**:
implement single layer perceptron linear model using function
"""

def model(X1_v,y1_v,w1_v):
  tf.compat.v1.disable_eager_execution()
  with session.graph.as_default():
   w1=tf.compat.v1.placeholder(tf.float32)
   b=tf.Variable([-0.2])
   X1=tf.compat.v1.placeholder(tf.float32)
   y1=tf.compat.v1.placeholder(tf.float32)
   init=tf.compat.v1.global_variables_initializer()
   session.run(init)
   linear_model=w1*X1+b
   squared_deltas=tf.square(linear_model-y1)
   loss=tf.reduce_sum(squared_deltas)
   linear_model=session.run(linear_model,feed_dict={X1:X1_v,w1:w1_v})
   squared_deltas=session.run(squared_deltas,feed_dict={X1:X1_v,y1:y1_v,w1:w1_v})
   loss_v=session.run(loss,feed_dict={X1:X1_v,y1:y1_v,w1:w1_v})
   print("loss value is :",loss_v,"\n")
  return linear_model,squared_deltas,loss_v


LM,SD,L=model([1,2,3,4],[0,-1,-2,-3],[0.2])
print(f"Linear model value is: {LM}\nSquared deltas value is :{SD}\nLoss values is : {L}")

"""#Implementation of Perceptron with Optimizer
-Error is 20.16 let's update the weights and check the error again
"""

w=tf.Variable([.2])
b=tf.Variable([-0.2])
x=tf.compat.v1.placeholder(tf.float32)
y=tf.compat.v1.placeholder(tf.float32) #Actual output
linear_model=w*x+b
init=tf.compat.v1.global_variables_initializer()
session.run(init)
print("predicted output is :",session.run(linear_model,{x:[1,2,3,4]}))
squared_deltas=tf.square(linear_model-y)
loss=tf.reduce_sum(squared_deltas)
print("loss value is :",session.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))
optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01)
train=optimizer.minimize(loss)
session.run(init)
for i in range(1000):
  session.run(train,{x:[1,2,3,4],y:[0,-1,-2,-3]})
print(session.run([w,b]))
print(session.run(linear_model,{x:[1,2,3,4],y:[0,-1,-2,-3]}))
print(session.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))

"""#Dynamic Bias"""

import tensorflow as tf
with session.graph.as_default():
 tf.compat.v1.disable_eager_execution()
 session=tf.compat.v1.Session()
 x=tf.compat.v1.placeholder(tf.float32)
 b=tf.compat.v1.placeholder(tf.float32)
 w=tf.Variable([2.0])
 y=w*x+b
 init=tf.compat.v1.global_variables_initializer()
 session.run(init)
 print(session.run(y,{x:[0,1,2,3,4],b:[1]}))





























































