# -*- coding: utf-8 -*-
"""Binary Classification Using Perceptron and Sonar dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BE5PUWxmKpBtv-9VCh_Y6IGiLqnnBWsR
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.utils import shuffle
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.model_selection import train_test_split
tf.compat.v1.disable_eager_execution()
session=tf.compat.v1.Session()

df=pd.read_csv('sonar.csv')

df.head()

df.info()

df.describe()

df.isnull().sum()

df.shape

df.columns

X=df[['0.02', '0.0371', '0.0428', '0.0207', '0.0954', '0.0986', '0.1539',
       '0.1601', '0.3109', '0.2111', '0.1609', '0.1582', '0.2238', '0.0645',
       '0.066', '0.2273', '0.31', '0.2999', '0.5078', '0.4797', '0.5783',
       '0.5071', '0.4328', '0.555', '0.6711', '0.6415', '0.7104', '0.808',
       '0.6791', '0.3857', '0.1307', '0.2604', '0.5121', '0.7547', '0.8537',
       '0.8507', '0.6692', '0.6097', '0.4943', '0.2744', '0.051', '0.2834',
       '0.2825', '0.4256', '0.2641', '0.1386', '0.1051', '0.1343', '0.0383',
       '0.0324', '0.0232', '0.0027', '0.0065', '0.0159', '0.0072', '0.0167',
       '0.018', '0.0084', '0.009', '0.0032']]
X

y=df['R']
y

y.value_counts()

y=LabelEncoder().fit_transform(y)
print(y)

y=y.reshape(-1,1)
Y=OneHotEncoder().fit_transform(y).toarray()
print(Y)

X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=0)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

learning_rate=0.01
training_epochs=100
cos_history=np.empty(shape=[1],dtype=float)
n_class=2

x=tf.compat.v1.placeholder(tf.float32,[None,60])
w=tf.Variable(tf.zeros([60,n_class]))
b=tf.Variable(tf.zeros([n_class]))
# after creating variables you should initialize them using global_variables_initializer
init=tf.compat.v1.global_variables_initializer()
session.run(init)
y=tf.compat.v1.placeholder(tf.float32,[None,n_class])
pred=tf.nn.softmax(tf.matmul(x,w)+b)  #very important
cost_func=tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(pred),axis=1)) #very important and understand later in detail
optimizer=tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(cost_func) #very important
mse_history=[]

"""#Iterating and running epochs"""

# let's print values by runing the session

for epoch in range(training_epochs):
    session.run(optimizer,feed_dict={x:X_train,y:y_train})
    cost=session.run(cost_func,feed_dict={x:X_train,y:y_train})
    cost_history=np.append(cos_history,cost)
    print(cost)
    y_pred=session.run(pred,feed_dict={x:X_test})
    mse=tf.reduce_mean(tf.square(y_pred-y_test))
    mse_history.append(session.run(mse))

print("MSE=",mse_history)