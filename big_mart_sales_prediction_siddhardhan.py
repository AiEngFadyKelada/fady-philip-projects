# -*- coding: utf-8 -*-
"""Big_Mart_Sales_Prediction_Siddhardhan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AazDygnscPj_DTUOp2YZyURgomodDeGZ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import r2_score,mean_absolute_percentage_error

big_mart_data=pd.read_csv('Train.csv')
big_mart_data.head()

big_mart_data

big_mart_data.shape

big_mart_data.info()

big_mart_data.describe()

big_mart_data.isnull().sum()

big_mart_data.columns.unique()

X=big_mart_data[['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',
       'Item_Type', 'Item_MRP', 'Outlet_Identifier',
       'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type',
       'Outlet_Type']]
y=big_mart_data['Item_Outlet_Sales']

X

y

len(big_mart_data['Item_Identifier'].unique())
df=pd.DataFrame(big_mart_data['Item_Identifier'].unique())
df.head(28)



"""* **1.(Data Categorization):**


Let's Identify which columns are numerical and wich which are categorical,as We later would need to convert categorical data into numbers for Label encoder
"""

numeric_cols=list(big_mart_data.select_dtypes('number').columns)
cat_cols=list(big_mart_data.select_dtypes('object').columns)

numeric_cols





cat_cols

big_mart_data[numeric_cols].isnull().sum()

big_mart_data[cat_cols].isna().sum()

big_mart_data['Outlet_Size'].unique()

big_mart_data['Outlet_Size'].nunique()

"""


# **2. Handling missing values in numerical columns through imputation**"""

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
imputer.fit(big_mart_data[numeric_cols])

big_mart_data[numeric_cols]=imputer.transform(big_mart_data[numeric_cols])

big_mart_data[numeric_cols]

big_mart_data[numeric_cols].isnull().sum()

"""# **3. Handling categorical  columns through Encoding Categorical Data**"""

big_mart_data[cat_cols].nunique()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
lencoded_data = {} # Use a dictionary to store encoded columns

for col in big_mart_data[cat_cols]:
     le.fit(big_mart_data[col])
     lencoded_data[col] = le.transform(big_mart_data[col])
lencoded_data=pd.DataFrame(lencoded_data)
lencoded_data



lencoded_data.isnull().sum()

new_df=pd.concat([big_mart_data[numeric_cols],lencoded_data],axis=1)
new_df

new_df.columns

X=new_df[['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',
       'Item_Type', 'Item_MRP', 'Outlet_Identifier',
       'Outlet_Establishment_Year' , 'Outlet_Size', 'Outlet_Location_Type',
       'Outlet_Type']]
y=new_df['Item_Outlet_Sales']

X

y

"""#**4.Validation_Train_Test**"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=0)
X_val,X_test,y_val,y_test=train_test_split(X,y,test_size=0.4,random_state=0)
print(X_train.shape,X_val.shape,X_test.shape)
print(y_train.shape,y_val.shape,y_test.shape)

?XGBRegressor

XGBR=XGBRegressor(n_estimators=85,learning_rate=1.2,max_depth=5,random_state=41)
XGBR.fit(X_train,y_train)
XGBR=XGBR.fit(X_train,y_train)

training_prediction = XGBR.predict(X_train)
training_prediction

# R squared Value
acc_score = r2_score(y_train, training_prediction)
print(acc_score)

XGBR.fit(X_val,y_val)
val_prediction = XGBR.predict(X_val)
val_prediction

r2_val = r2_score(y_val, val_prediction)
print(r2_val)

XGBR.fit(X_test,y_test)
test_prediction = XGBR.predict(X_test)
test_prediction

r2_test = r2_score(y_test, test_prediction)
print(r2_test)
#from trying different learning rates in the previous model we noticed the best train value and validation value are at learning rate of 1.2
#random_state=41
#max_depth=5
#n_estimators=85

def test_params(**params):
  XGBR=XGBRegressor(**params,n_jobs=-1,random_state=41,learning_rate=1.2,max_depth=5)
  XGBR.fit(X_train,y_train)
  train_prediction=XGBR.predict(X_train)
  r2_train = r2_score(y_train, training_prediction)

  XGBR.fit(X_val,y_val)
  val_prediction=XGBR.predict(X_val)
  r2_val = r2_score(y_val, val_prediction)

  XGBR.fit(X_test,y_test)
  test_prediction=XGBR.predict(X_test)
  r2_test = r2_score(y_test, test_prediction)

  return{'parameters':params,'Training Accuracy':r2_train,'Validation Accuracy':r2_val,'Test Accuracy':r2_test}



df_estimators=pd.DataFrame([test_params(n_estimators=n) for n in range(50,100,5)])
df_estimators


#best max depth = 5

new_data=pd.read_csv('Test.csv')
new_data

def predict_input(data):
  num_cols=list(data.select_dtypes('number').columns)
  cat_cols=list(data.select_dtypes('object').columns)
  imputer=SimpleImputer(strategy='mean')
  imputer.fit(data[num_cols])
  data[num_cols]=imputer.transform(new_data[num_cols])
  lencoded_data2 = {} # Use a dictionary to store encoded columns
  for col in data[cat_cols]:
    le.fit(data[col])
    lencoded_data2[col]=le.transform(data[col])
  lencoded_data2=pd.DataFrame(lencoded_data2)
  Input_Data=pd.concat([data[num_cols],lencoded_data2],axis=1)
    # Get the correct order of columns from XGBR
  correct_order = XGBR.get_booster().feature_names
  Input_Data = Input_Data[correct_order] # Reorder columns
  # Make predictions
  Prediction_Data=XGBR.predict(Input_Data)
  Prediction_Data=pd.DataFrame(Prediction_Data)
  return Prediction_Data

data2=predict_input(new_data)
data2 = data2.rename(columns={data2.columns[0]:'Y-hat'})
data2

def function_regressor1(item_id,item_type,data):
 for i in range(len(new_data.index)):
    if new_data['Item_Identifier'][i]==item_id and new_data['Item_Type'][i]==item_type:
       y=dict(new_data.iloc[i])
       y=pd.DataFrame(y,index=[0])
       # Access the first column of Data using iloc
       d2=pd.DataFrame([data.iloc[i]],index=[0])
       d2 = d2.rename(columns={d2.columns[0]: 'Prediction'})
       k=pd.concat([y,d2],axis=1)
       return k
       break
    elif i==len(new_data.index)-1:
       print('not found')
       return None

new_df=function_regressor1('FDB58','Snack Foods',data2)
new_df

def predict_input(data):
  num_cols=list(data.select_dtypes('number').columns)
  cat_cols=list(data.select_dtypes('object').columns)
  imputer=SimpleImputer(strategy='mean')
  imputer.fit(data[num_cols])
  data[num_cols]=imputer.transform(new_data[num_cols])
  lencoded_data2 = {} # Use a dictionary to store encoded columns
  for col in data[cat_cols]:
    le.fit(data[col])
    lencoded_data2[col]=le.transform(data[col])
  lencoded_data2=pd.DataFrame(lencoded_data2)
  Input_Data=pd.concat([data[num_cols],lencoded_data2],axis=1)
    # Get the correct order of columns from XGBR
  correct_order = XGBR.get_booster().feature_names
  Input_Data = Input_Data[correct_order] # Reorder columns
  # Make predictions
  Prediction_Data=XGBR.predict(Input_Data)
  Prediction_Data=pd.DataFrame(Prediction_Data)
  return Prediction_Data




def function_regressor1(item_id,item_type,data):
 for i in range(len(new_data.index)):
    if new_data['Item_Identifier'][i]==item_id and new_data['Item_Type'][i]==item_type:
       y=dict(new_data.iloc[i])
       y=pd.DataFrame(y,index=[0])
       # Access the first column of Data using iloc
       d2=pd.DataFrame([data.iloc[i]],index=[0])
       d2 = d2.rename(columns={d2.columns[0]: 'Prediction'})
       k=pd.concat([y,d2],axis=1)
       return k
       break
    elif i==len(new_data.index)-1:
       print('not found')
       return None


data2=predict_input(new_data)
new_df=function_regressor1('FDB58','Snack Foods',data2)

data2

data2 = data2.rename(columns={data2.columns[0]:'Y-hat'})
data2

new_df





