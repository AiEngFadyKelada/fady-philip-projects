# -*- coding: utf-8 -*-
"""Random Forest Classifier and Hyperparameters Tuning Jovian Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iOgxDt1HX06kqUq-hZH1hLdi1cRpEFGq
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.model_selection import train_test_split

df=pd.read_csv('weatherAUS.csv')
df.head()

df.info()

df.describe()

df.isnull().sum()

df.shape

df.columns

df.dropna(subset=['RainToday','RainTomorrow'],inplace=True)

X=df[['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',
       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',
       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',
       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',
       'Temp3pm', 'RainToday']]
y=df['RainTomorrow']
X

numeric_cols=df.select_dtypes(include=np.number).columns
numeric_cols

categorical_cols=df.select_dtypes(include='object').columns
categorical_cols

df[categorical_cols].isna().sum()

df[numeric_cols].isna().sum()

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
imputer.fit(df[numeric_cols])

imputer.transform(df[numeric_cols])

df[numeric_cols]=imputer.transform(df[numeric_cols])

df[categorical_cols].nunique()

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
encoder=OneHotEncoder(sparse=False,handle_unknown='ignore')

encoder.fit(df[['RainToday','RainTomorrow']])

encoder.categories_

categorical_cols2=['RainToday','RainTomorrow']
encoded_cols = list(encoder.get_feature_names_out(categorical_cols2))
encoded_cols
df[encoded_cols] = encoder.transform(df[['RainToday','RainTomorrow']])

df.drop(columns=['RainToday','RainTomorrow'],inplace=True)

le = LabelEncoder()
for col in ['Location','WindGustDir','WindDir9am','WindDir3pm']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])

categorical_cols

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit(df[numeric_cols])
df[numeric_cols]=scaler.transform(df[numeric_cols])

df.columns

X=df[['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',
       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',
       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',
       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',
       'Temp3pm', 'RainToday_No', 'RainToday_Yes']]

y=df[[ 'RainTomorrow_No', 'RainTomorrow_Yes']]

X

from sklearn.model_selection import train_test_split
X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=45)
X_val,X_test,y_val,y_test=train_test_split(X_val,y_val,test_size=0.2,random_state=42)

from sklearn.ensemble import RandomForestClassifier
base_model=RandomForestClassifier(n_jobs=-1,random_state=42)
base_model.fit(X_train,y_train)

X_train_pred=base_model.predict(X_train)
base_train_acc=accuracy_score(y_train,X_train_pred)
print(base_train_acc)
X_val_pred=base_model.predict(X_val)
base_val_acc=accuracy_score(y_val,X_val_pred)
print(base_val_acc)

number_of_decisiontrees=base_model.estimators_
len(number_of_decisiontrees)

base_model.estimators_[0]

?RandomForestClassifier

def n_est(n):
  rf=RandomForestClassifier(n_estimators=n,n_jobs=-1,random_state=42)
  rf.fit(X_train,y_train)
  X_train_pred=rf.predict(X_train)
  rf_train_acc=accuracy_score(y_train,X_train_pred)
  X_val_pred=rf.predict(X_val)
  rf_val_acc=accuracy_score(y_val,X_val_pred)
  train_error=1-rf_train_acc
  val_error=1-rf_val_acc
  return{'n_estimators':n,'Training error':train_error,'Validation error':val_error}




error_df=pd.DataFrame([n_est(n) for n in range(1,20)])
error_df
#best n_estimators=1 or 4

rf=RandomForestClassifier(n_estimators=4,n_jobs=-1,random_state=42)
  rf.fit(X_train,y_train)
  X_train_pred=rf.predict(X_train)
  rf_train_acc=accuracy_score(y_train,X_train_pred)
  X_val_pred=rf.predict(X_val)

plt.figure()
plt.plot(error_df['n_estimators'], error_df['Training error'])
plt.plot(error_df['n_estimators'], error_df['Validation error'])
plt.title('Training vs. Validation error')
plt.xticks(range(1,23, 1))
plt.xlabel('n_estimators')
plt.ylabel('Prediction error (1 - Accuracy)')
plt.legend(['Training', 'Validation'])

def test_params(**params):
  rf=RandomForestClassifier(**params,n_jobs=-1,random_state=42)
  rf.fit(X_train,y_train)
  X_train_pred=rf.predict(X_train)
  train_acc=accuracy_score(y_train,X_train_pred)
  X_val_pred=rf.predict(X_val)
  val_acc=accuracy_score(y_val,X_val_pred)
  train_error=1-train_acc
  val_error=1-val_acc
  return{'parameters':params,'Training error':train_error,'Validation error':val_error}



df_max_depth=pd.DataFrame([test_params(max_depth=n) for n in range(1,50)])
df_max_depth

df_max_depth['parameters']=[i for i in range(1,50)]
df_max_depth

plt.figure()
plt.plot(df_max_depth['parameters'], df_max_depth['Training error'])
plt.plot(df_max_depth['parameters'], df_max_depth['Validation error'])
plt.title('Training vs. Validation error')
plt.xticks(range(1,50, 2))
plt.xlabel('max_depth')
plt.ylabel('Prediction error (1 - Accuracy)')
plt.legend(['Training', 'Validation'])

#best max_depth=11

def test_params(**params):
  rf=RandomForestClassifier(**params,n_jobs=-1,random_state=42)
  rf.fit(X_train,y_train)
  X_train_pred=rf.predict(X_train)
  train_acc=accuracy_score(y_train,X_train_pred)
  X_val_pred=rf.predict(X_val)
  val_acc=accuracy_score(y_val,X_val_pred)
  train_error=1-train_acc
  val_error=1-val_acc
  return{'parameters':params,'Training error':train_error,'Validation error':val_error}



df_max_leaf=pd.DataFrame([test_params(max_leaf_nodes=n) for n in range(2,4000,50)])
df_max_leaf

df_max_leaf['parameters']=[i for i in range(2,4000,50)]
df_max_leaf

plt.figure()
plt.plot(df_max_leaf['parameters'], df_max_leaf['Training error'])
plt.plot(df_max_leaf['parameters'], df_max_leaf['Validation error'])
plt.title('Training vs. Validation error')
plt.xticks(range(0,4000, 300))
plt.xlabel('max_leaf')
plt.ylabel('Prediction error (1 - Accuracy)')
plt.legend(['Training', 'Validation'])

#best max_leaf_nodes=277









def test_params(**params):
  rf=RandomForestClassifier(**params,n_jobs=-1,random_state=42)
  rf.fit(X_train,y_train)
  X_train_pred=rf.predict(X_train)
  train_acc=accuracy_score(y_train,X_train_pred)
  X_val_pred=rf.predict(X_val)
  val_acc=accuracy_score(y_val,X_val_pred)
  train_error=1-train_acc
  val_error=1-val_acc
  return{'parameters':params,'Training error':train_error,'Validation error':val_error}



df_min_samples_leaf=pd.DataFrame([test_params(min_samples_leaf=n) for n in range(1,1000,10)])
df_min_samples_leaf



df_min_samples_leaf['parameters']=[i for i in range(1,1000,10)]
df_min_samples_leaf

plt.figure()
plt.plot(df_min_samples_leaf['parameters'], df_min_samples_leaf['Training error'])
plt.plot(df_min_samples_leaf['parameters'], df_min_samples_leaf['Validation error'])
plt.title('Training vs. Validation error')
plt.xticks(range(1,1000, 70))
plt.xlabel('min_samples_leaf')
plt.ylabel('Prediction error (1 - Accuracy)')
plt.legend(['Training', 'Validation'])

#best min smaples leaf=171



def test_params(**params):
  rf=RandomForestClassifier(**params,n_jobs=-1,random_state=42)
  rf.fit(X_train,y_train)
  X_train_pred=rf.predict(X_train)
  train_acc=accuracy_score(y_train,X_train_pred)
  X_val_pred=rf.predict(X_val)
  val_acc=accuracy_score(y_val,X_val_pred)
  train_error=1-train_acc
  val_error=1-val_acc
  return{'parameters':params,'Training error':train_error,'Validation error':val_error}



df_min_samples_split=pd.DataFrame([test_params(min_samples_split=n) for n in range(2,600,20)])
df_min_samples_split

df_min_samples_split['parameters']=[i for i in range(2,600,20)]
df_min_samples_split

#best min samples split=250

plt.figure()
plt.plot(df_min_samples_split['parameters'], df_min_samples_split['Training error'])
plt.plot(df_min_samples_split['parameters'], df_min_samples_split['Validation error'])
plt.title('Training vs. Validation error')
plt.xticks(range(0,600, 50))
plt.xlabel('min_samples_slpit')
plt.ylabel('Prediction error (1 - Accuracy)')
plt.legend(['Training', 'Validation'])

def test_params(**params):
  rf=RandomForestClassifier(**params,n_jobs=-1,random_state=42)
  rf.fit(X_train,y_train)
  X_train_pred=rf.predict(X_train)
  train_acc=accuracy_score(y_train,X_train_pred)
  X_val_pred=rf.predict(X_val)
  val_acc=accuracy_score(y_val,X_val_pred)
  train_error=1-train_acc
  val_error=1-val_acc
  return{'parameters':params,'Training error':train_error,'Validation error':val_error}



df_min_samples_split=pd.DataFrame([test_params(min_samples_split=n) for n in range(2,600,20)])
df_min_samples_split

rf2=RandomForestClassifier(n_jobs=-1,random_state=2,min_impurity_decrease=1e-4)
rf2.fit(X_train,y_train)
X_train_pred=rf2.predict(X_train)
rf2_train_acc=accuracy_score(y_train,X_train_pred)
X_val_pred=rf2.predict(X_val)
rf2_val_acc=accuracy_score(y_val,X_val_pred)
print(rf2_train_acc,rf2_val_acc)

#best min_impurity_decrease=1e-4



Best_model = RandomForestClassifier(n_jobs=-1,
                               random_state=2,
                               n_estimators=4,
                               max_leaf_nodes=277,
                               max_depth=11,
                               min_samples_leaf=171,
                               min_samples_split=250,
                               min_impurity_decrease=1e-4)

Best_model.fit(X_train, y_train)
print(Best_model.score(X_train, y_train))
print(Best_model.score(X_val, y_val))



