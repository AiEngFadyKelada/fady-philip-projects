# -*- coding: utf-8 -*-
"""Rooms Classification Project using CNNs Tensorflows.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AHKAwGRTDglk5LIBVJImaP98pqm5f1_N
"""

import numpy as np
import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import matplotlib.pyplot as plt
import pandas as pd
import os
import cv2
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix

room_types=os.listdir('dataset')
room_types.remove('.ipynb_checkpoints')
print(room_types)
rooms=[]
for item in room_types:
  all_rooms=[j for j in os.listdir('dataset/'+item) if j.endswith('.jpg')]
  for room in all_rooms:
    rooms.append((item,str('dataset' +'/'+item) + '/' +room))
print(rooms)

rooms_df=pd.DataFrame(data=rooms,columns=['room_types','images'])
print("total number of rooms in df:",len(rooms_df))

rooms_df

rooms_count=rooms_df['room_types'].value_counts()
print('rooms in each catefory',rooms_count)

im_size=60
images=[]
labels=[]
path='dataset/'
for i in room_types:
  data_path=path+str(i)
  filenames=[i for i in os.listdir(data_path)]
  for f in filenames:
    img=cv2.imread(data_path+'/'+f )
    img=cv2.resize(img,(im_size,im_size))
    images.append(img)
    labels.append(i)
print(images,'\n\n')
print(labels)

images=np.array(images)
images.shape

y=rooms_df['room_types'].values
LE=LabelEncoder()
y=LE.fit_transform(y)
y=y.reshape(-1,1)
OHE=OneHotEncoder(categories='auto')
y=OHE.fit_transform(y)
print(y)

y.shape

images,y=shuffle(images,y,random_state=1)
X_train,X_test,y_train,y_test=train_test_split(images,y,test_size=0.2,random_state=1)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

num_classes=3
n_input=10800
learning_rate=0.001
training_iters=10
batch_size=16
display_step=20
num_channels=3
x=tf.placeholder(tf.float32,([None,im_size,im_size,num_channels]))
                            #batch_size
y=tf.placeholder(tf.float32,([None,num_classes]))
                            #batch_size
print('shape of placeholder:',x.shape,y.shape)

#Convolutional layer and reLu layer
def conv2d(x,w,b,strides=1):
  x=tf.nn.conv2d(x,w,strides=[1,strides,strides,1],padding='SAME') #xw
  x=tf.nn.bias_add(x,b) #summation: xw + 'b'
  x=tf.nn.relu(x)  #activation function
  return x

#pooling layer
def maxpool2d(x,k=2):
  x=tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')
  return x

print(x)

weights={
    'w1':tf.Variable(tf.random_normal([5,5,3,32],name='w1')),
    'w2':tf.Variable(tf.random_normal([5,5,32,64],name='w2')),
    'w3':tf.Variable(tf.random_normal([5,5,64,128],name='w3')),
    'wd1':tf.Variable(tf.random_normal([8*8*128,2048],name='wd1')),
    'wout':tf.Variable(tf.random_normal([2048,num_classes],name='wd1'))
}

biases={
    'b1':tf.Variable(tf.random_normal([32],name='b1')),
    'b2':tf.Variable(tf.random_normal([64],name='b2')),
    'b3':tf.Variable(tf.random_normal([128],name='b3')),
    'bd1':tf.Variable(tf.random_normal([2048],name='bd1')),
    'bout':tf.Variable(tf.random_normal([num_classes]),name='bout')
}

def conv_net(x,weights,biases):
  # reshape input to 60*60*3 size
  x=tf.reshape(x,shape=[-1,60,60,3])
  conv1=conv2d(x,weights['w1'],biases['b1'])
  pool1=maxpool2d(conv1,k=2)
  conv2=conv2d(pool1,weights['w2'],biases['b2'])
  pool2=maxpool2d(conv2,k=2)
  conv3=conv2d(pool2,weights['w3'],biases['b3'])
  pool3=maxpool2d(conv3,k=2)
  #input is sized as 8*8*128

  fc1=tf.reshape(conv3,[-1,weights['wd1'].get_shape().as_list()[0]])
  print(fc1)
  # The tf.matmul function only takes two required arguments: a and b.
  # 'transpose_a' is an optional argument, and it should be a boolean.
  # You likely want to add the biases after the matrix multiplication.
  fc1=tf.add(tf.matmul(fc1,weights['wd1']),biases['bd1'])
  fc1=tf.nn.relu(fc1)
  print(fc1.shape)
  # Similarly, you likely want to add the output biases after the multiplication.
  output=tf.add(tf.matmul(fc1,weights['wout']),biases['bout'])
  return output

model=conv_net(x,weights,biases)
print(model)

cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model,labels=y))
optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

init=tf.global_variables_initializer()

cost_history=[]
n_epochs=10

sess=tf.Session()
sess.run(init)



for epoch in range(n_epochs):
    # Convert X_train and y_train to dense NumPy arrays before feeding
    op, co = sess.run([optimizer, cost], feed_dict={x: X_train, y: y_train})
    cost_history.append(co)
    print('epoch:', epoch + 1, 'cost:', co)

